library(ggplot2)
library(viridis)
library(inlabru)
library(INLA)
library(dplyr)
library(data.table)
library(metR)
library(matrixStats)
library(parallel)

# function to calculate the value of the Omori's law at given time

# input : tt --> time at which evaluate the function
#         c.p, pm1 --> parameters of Omori's law (pm1 correspond to p.p - 1)
#         Ht --> observed time poits
#         ll0 --> values for the approximation to ensure stability

# output : numeric - value of the UNNORMALIZED Omori's law h(tt - Ht, c, p)
# notes : only one of parameters can be a vector, Ht has to be sorted (increas.) if 
#         a vector

trig.unnorm <- function(tt, c.p, pm1, Ht.ts, ll0 = -600){
  # compute only for positive time differeces
  diffs <- tt - Ht.ts
  idx.p <- tt - Ht.ts > 0
  logtrig <- (-1)*(pm1 + 1)*log(diffs[idx.p] + c.p)
  
  # approximating for large negative logtrig
  trig = logtrig
  idx <- logtrig < ll0 
  trig[idx] <- exp(ll0)*(1 - (-logtrig[idx] + ll0))
  trig[!idx] <- exp(logtrig[!idx])
  
  toreturn <- rep(0, length(diffs))
  toreturn[idx.p] <- trig
  return(toreturn)
}




# function to calculate the integral of the time-triggering function
# between Ht and Tlim. It can be seen as the probability that a point generated by Ht
#  is smaller than Tlim

# input : Tlim --> Extreme of the interval of time on which calculate the integral
#         c.p , pm1 --> parameters of the Omori's law (c.p > 0 , pm1 = p - 1 > 0)
#         Ht --> observed time points 

# output : numeric - The integral of the UNNORMALIZED Omori's law 
# notes : reliable only if AT MOST one input is a vector

I.h.unnorm <- function(Tlim, c.p, pm1, Ht.ts){
  (1/pm1)*(c.p^(-pm1) - (Tlim - Ht.ts + c.p)^(-pm1))
}



####################################################################
# I THIK THAT WE CAN USE THE SAMPLING ALREADY WRITTEN for the time.#
####################################################################


# function that computes the inverse with respect to Tlim of the I.h, meaning that 
# if I.h(Tlim,...) = u then Inv.h.cdf(u,...) = Tlim
# input : u --> value of the integral 
#         c.p, p.p --> parameters of Omori's law
#         Ht.ts --> observed time points (no mags)
# output : Tlim 
# notes : reliable only if AT MOST one input is a vector

Inv.h.cdf <- function(u, c.p, pm1, Ht.ts){
  c.p*((1 - u)^(1/(-pm1))) + Ht.ts - c.p 
}



Inv.h.unnorm <- function(Lambda, c.p, pm1, Ht.ts){
  (c.p^(-pm1) + Lambda*(-pm1))^(-1/pm1) + Ht.ts - c.p 
}


# function to sample n points smaller as generated by Ht following a distrib.
#   given by Omori's law (only points smaller than Tlim are retained).
#   we use inverse sampling technique 

# input: n.ev --> number of points to sample from Omori's law
#        c.p ,p.p --> parameters of Omori's law
#        Ht --> observation "generating" the sample (single value!)
#        Tlim --> Time limit to filter the sample

# output: numeric - vector of values between (Ht, Tlim) distribuited accordingly to 
#                   Omori's law with parameters c.p, p.p

sample.omori <- function(n.ev, c.p, pm1, Ht.ts, Tlim){
  if(n.ev == 0){return('no samples')}
  u <- runif(n.ev)
  t.sample <- Inv.h.cdf(u, c.p, pm1, Ht.ts)
  if(all(t.sample > Tlim)){return('no samples')}
  sort(t.sample[t.sample < Tlim])
}



sample.omori.unnorm <- function(n.ev, c.p, pm1, Ht.ts, Tlim){
  if(n.ev == 0){return('no samples')}
  bound <- I.h.unnorm(Tlim, c.p, pm1, Ht.ts)
  ss <- runif(n.ev, min = 0, max = bound)#I.h.unnorm(4, c.p, pm1, Ht.ts))#rexp(n.ev, 1)
  t.sample <- Inv.h.unnorm(ss, c.p, pm1, Ht.ts)
  if(all(t.sample > Tlim)){return('no samples')}
  sort(t.sample[t.sample < Tlim])
}



# function to sample from hawkes temporal process (time triggering given by Omori's law)
#   between 0 and Tlim

# input : params --> vector of parameters of the hawkes process (mu, K, alpha, c, p)
#         Tlim --> end of time interval
#         n.core --> number of cores to be used

# output : data.frame with 2 columns :
#                     1) ts = sampled observations (sorted)
#                     2) gen = generation of the event (0 = background,
#                                                       1 = gen by bckg,
#                                                       2 = gen by 1,
#                                                       ...)

# notes: we are assuming no observations before 0 and no known points in (0,T)

sample.ETAS.unnorm <- function(theta.params, beta.par, M0, Tlim, n.core = 5){
  theta1 <- theta.params[1]
  theta2 <- theta.params[2]
  theta3 <- theta.params[3]
  theta4 <- theta.params[4]
  theta5 <- theta.params[5]
  
  # extract number of backgroud events
  n.backg <- rpois(1, lambda = exp(theta1)*Tlim)
  print(c(n.backg, 0))
  # extract times homogeneously
  backg <- runif(n.backg, min = 0, max = Tlim)
  
  Past.Gens <- list(data.frame(ts = sort(backg),
                               mags = rexp(n.backg, beta.par) + M0,
                               gen = 0))
  flag = T
  gen.idx = 1
  while(flag){
    # take previous generation
    past.ts <- Past.Gens[[gen.idx]]$ts
    past.ms <- Past.Gens[[gen.idx]]$mags
    
    n.past.ts <- length(past.ts)
    
    # for each of them sample the number of offsprings
    Lambdas <- exp(theta2)*exp(exp(theta3)*(past.ms - M0))*I.h.unnorm(Tlim,
                                                                       exp(theta4),
                                                                       exp(theta5),
                                                                       past.ts)
    n.off <- rpois(n.past.ts, lambda = Lambdas)
    
    # exit condition if none of the event in the previous generation has
    # offsprings
    if(all(n.off == 0)){
      Gens.df <- bind_rows(Past.Gens)
      Gens.df <- Gens.df[order(Gens.df$ts),]
      return(Gens.df)
    }
    # select only times which has generated events
    past.ts <- past.ts[n.off > 0]
    n.off <- n.off[n.off > 0]
    
    # generate offsprings for each past event
    offsprings.list <- mclapply(1:length(past.ts), function(x)
      data.frame(ts = sample.omori.unnorm(n.off[x], exp(theta4), exp(theta5), 
                                          past.ts[x], Tlim),
                 mags = rexp(n.off[x], beta.par) + M0,
                 gen = gen.idx), mc.cores = n.core)
    offsprings.df <- rbindlist(offsprings.list)
    
    offsprings.df <-
      offsprings.df %>%
      filter(ts != 'no samples') %>%
      mutate(ts = as.numeric(ts))
    
    print(c(nrow(offsprings.df), gen.idx))
    # update gen.idx
    gen.idx <- gen.idx + 1
    Past.Gens[[gen.idx]] <- offsprings.df
  }
}



# function to compute the log-intesity of the Hawkes process with Omori's temporal 
#  triggering for a single time point.

# input : ts --> time at which evaluate the log-intesity (single value)
#         params --> vector of parameters using pm1 = p - 1
#         Ht --> observed time points (has to be a numeric and sorted)

# output : numeric (single value) value of the log-intesity

# notes : In params we use pm1 = p - 1, ts has to be a single value

log.lambda.unnorm.s <- function(ts, theta.params, Ht, M0){
  theta1 <- theta.params[1]
  theta2 <- theta.params[2]
  theta3 <- theta.params[3]
  theta4 <- theta.params[4]
  theta5 <- theta.params[5]
  
  
  # triggers from each past event
  triggers <- sum(exp(exp(theta3)*(Ht[,2] - M0))*
                    trig.unnorm(ts, exp(theta4), exp(theta5), Ht[,1]))
  
  # return
  logSumExp(c(theta1, theta2 + log(triggers)))
  # unstable
  #print(log(mu.p + k.p*triggers))
  
}


# function to compute the log-intesity of the Hawkes process with Omori's temporal 
#  triggering for a vector of values of t

# input : ts.v --> vector of times at which evaluate the log-intesity 
#         params --> vector of parameters using pm1 = p - 1
#         Ht --> observed time points (has to be a numeric and sorted)

# output : numeric (vector) value of the log-intesity evaluated at ts.v

log.lambda.unnorm <- function(ts.v, theta.params, Ht, M0){
  sapply(ts.v, function(t) log.lambda.s.unnorm(t, theta.params, Ht, M0))
}



# here Ht.ts <= T1 otherwise it gets NA

# integral between T1 and T2 of h(t - t_i) where t > t_i
I.h.indef <- function(T1, T2, th4, th5, Ht.ts){
  if(any(Ht.ts > T1)){
    stop('Error T1 < t_i')
  }
  
  (1/exp(th5))*((T1 - Ht.ts + exp(th4))^(-exp(th5)) - 
                  (T2 - Ht.ts + exp(th4))^(-exp(th5)))

}


I.h.indef <- function(T1, T2, th4, th5, Ht.ts){
  if(any(Ht.ts > T1)){
    stop('Error T1 < t_i')
  }
  
  logq <- log((1/exp(th5))) + log((T1 - Ht.ts + exp(th4))^(-exp(th5)) - 
                  (T2 - Ht.ts + exp(th4))^(-exp(th5)))
  
  return(exp(logq))
}




Lambda.s <- function(T1, T2, theta.params, Ht, M0){
  if(T2 < T1){
    stop("T2 cant be greater than T1")
  }  
  theta1 <- theta.params[1]
  theta2 <- theta.params[2]
  theta3 <- theta.params[3]
  theta4 <- theta.params[4]
  theta5 <- theta.params[5]
  flag.pre <- FALSE
  flag.bet <- FALSE
  
  
  bkg <- exp(theta1)*(T2 - T1)
  idx.pre <- Ht[,1] < T1
  
  if(sum(idx.pre) > 0){
    flag.pre = TRUE
    Ht.pre <- Ht[idx.pre, ]
    
    Ht.pre$Ih <- I.h.indef(T1, T2, theta4, theta5, Ht.pre$ts) 
      
    Ht.pre$Trig.int = exp(exp(theta3)*(Ht.pre$mags - M0))*Ht.pre$Ih
  }
  
  
  idx.between <- Ht[,1] >= T1 & Ht[,1] < T2
  
  if(sum(idx.between) > 0){
    flag.bet = TRUE
    Ht.between <- Ht[idx.between, ]
    
    Ht.between$Ih <- I.h.indef(Ht.between$ts, T2, theta4, theta5, Ht.between$ts)
    Ht.between$Trig.int = exp(exp(theta3)*(Ht.between$mags - M0))*Ht.between$Ih
    
  }
  
  
  if(flag.bet & flag.pre){
    #print('both')
    # events both before T1 and in (T1,T2)
    Ht.f <- rbind(Ht.pre, Ht.between)
    return(bkg + exp(theta2)*sum(Ht.f$Trig.int))
  }
  
  if(flag.bet & !flag.pre){
    #print('only between')
    # events only in (T1,T2) no before T1
    Ht.f <- Ht.between
    return(bkg + exp(theta2)*sum(Ht.f$Trig.int))
  }
  
  if(!flag.bet & flag.pre){
    #print('only before')
    # events only before T1 no in (T1, T2)
    Ht.f <- Ht.pre
    return(bkg + exp(theta2)*sum(Ht.f$Trig.int))
  }
  
  if(!flag.bet & !flag.pre){
    #print('no events')
    # no events before T2
    return(bkg)
  }
}


# it assumes it starts from 0
toplot.cumcounts <- function(Tlim, theta.par, Ht, M0, by.s){
  t.breaks <- seq(0, Tlim, by = by.s)
  LL = c()
  n.o = c()
  for(i in 1:(length(t.breaks) - 1)){
    t1 = t.breaks[i]
    t2 = t.breaks[i+1]
    LL[i] = Lambda.s(t1, t2, theta.par, Ht[Ht[,1] < t2,], M0)
    n.o[i] = sum(Ht[,1] >= t1 & Ht[,1] < t2)
  }
  t.mids <- t.breaks[-length(t.breaks)] + by.s/2
  df <- rbind(data.frame(counts = cumsum(n.o),
                         times = t.mids,
                         type = 'observed'),
              data.frame(counts = cumsum(LL),
                         times = t.mids,
                         type = 'expected'))
  ggplot(df, aes(x = times, y = counts, color = type, linetype = type)) + 
    geom_step()
}


toplot.cumcounts <- function(Tlim, theta.par, Ht, M0, by.s){
  t.breaks <- seq(0, Tlim, by = by.s)
  LL = c()
  n.o = c()
  for(i in 1:(length(t.breaks) - 1)){
    t1 = t.breaks[i]
    t2 = t.breaks[i+1]
    LL[i] = Lambda.s(t1, t2, theta.par, Ht[Ht[,1] < t2,], M0)
    n.o[i] = sum(Ht[,1] >= t1 & Ht[,1] < t2)
  }
  t.mids <- t.breaks[-length(t.breaks)] + by.s/2
  df <- rbind(data.frame(counts = cumsum(n.o),
                         times = t.mids,
                         type = 'observed'),
              data.frame(counts = cumsum(LL),
                         times = t.mids,
                         type = 'expected'))
  ggplot(df, aes(x = times, y = counts, color = type, linetype = type)) + 
    geom_step()
}








log.lambda.ETAS.single <- function(tt, theta.par, Ht, M0){
  theta1 <- theta.par[1]
  theta2 <- theta.par[2]
  theta3 <- theta.par[3]
  theta4 <- theta.par[4]
  theta5 <- theta.par[5]
  
  past.idx <- Ht[,1] < tt
  past.ts <- Ht[past.idx, 1]
  past.ms <- Ht[past.idx, 2]
    

  logs.trig.comp <- theta2 + exp(theta3)*(past.ms - M0) + 
    (-exp(theta5) - 1)*log(tt - past.ts + exp(theta4))
  
  log.trig <- logSumExp(logs.trig.comp)
  
  #print(exp(log.trig))
  # in case it turns out to not be stable, we can approximate the exponential
  # easily for huge/tiny values of logSumExp
  logSumExp(c(theta1, log.trig))
}


log.lambda.ETAS <- function(ts, theta.par, Ht, M0){
  sapply(ts, function(x) log.lambda.ETAS.single(x, theta.par, Ht, M0))
}

ETAS.log.lik <- function(theta.par, Ht, M0, Tlim){
  exp.v <- Lambda.s(0, Tlim, theta.par, Ht, M0)
  sum.c <- sum(log.lambda.ETAS(Ht[,1], theta.par, Ht, M0))
  -exp.v + sum.c
}

ETAS.log.lik.toptim <- function(theta.par, Ht, M0, Tlim){
  -ETAS.log.lik(theta.par, Ht, M0, Tlim)
}


##########
# theta1 #
##########

log.lambda.der.th1.single <- function(theta.par, tt, Ht, M0){
  ll0 <- exp(log.lambda.ETAS(tt, theta.par, Ht, M0))
  
  exp(theta.par[1])/ll0
}


log.lambda.der.th1 <- function(theta.par, ts, Ht, M0){
  sapply(ts, function(x) log.lambda.der.th1.single(theta.par, x, Ht, M0))
}


##########
# theta2 #
##########

log.lambda.der.th2.single <- function(theta.par, tt, Ht, M0){
  
  ll0 <- exp(log.lambda.ETAS(tt, theta.par, Ht, M0))
  
  theta2 <- theta.par[2]
  theta3 <- theta.par[3]
  theta4 <- theta.par[4]
  theta5 <- theta.par[5]
  
  past.idx <- Ht[,1] < tt
  past.ts <- Ht[past.idx, 1]
  past.ms <- Ht[past.idx, 2]
  
  
  logs.trig.comp <- theta2 + exp(theta3)*(past.ms - M0) + 
    (-exp(theta5) - 1)*log(tt - past.ts + exp(theta4))
  
  log.trig <- logSumExp(logs.trig.comp)
  
  exp(log.trig)/ll0
}

log.lambda.der.th2 <- function(theta.par, ts, Ht, M0){
  sapply(ts, function(x) log.lambda.der.th2.single(theta.par, x, Ht, M0))
}


##########
# theta3 #
##########


log.lambda.der.th3.single <- function(theta.par, tt, Ht, M0){
  
  ll0 <- exp(log.lambda.ETAS(tt, theta.par, Ht, M0))
  
  theta2 <- theta.par[2]
  theta3 <- theta.par[3]
  theta4 <- theta.par[4]
  theta5 <- theta.par[5]
  
  past.idx <- Ht[,1] < tt
  past.ts <- Ht[past.idx, 1]
  past.ms <- Ht[past.idx, 2]
  
  logs.trig.comp <- theta2 + exp(theta3)*(past.ms - M0) + theta3 + 
    log(past.ms - M0) +  
    (-exp(theta5) - 1)*log(tt - past.ts + exp(theta4))
  
  log.trig <- logSumExp(logs.trig.comp)
  
  exp(log.trig)/ll0
}

log.lambda.der.th3 <- function(theta.par, ts, Ht, M0){
  sapply(ts, function(x) log.lambda.der.th3.single(theta.par, x, Ht, M0))
}


##########
# theta4 #
##########


log.lambda.der.th4.single <- function(theta.par, tt, Ht, M0){
  
  ll0 <- exp(log.lambda.ETAS(tt, theta.par, Ht, M0))
  
  theta2 <- theta.par[2]
  theta3 <- theta.par[3]
  theta4 <- theta.par[4]
  theta5 <- theta.par[5]
  
  past.idx <- Ht[,1] < tt
  past.ts <- Ht[past.idx, 1]
  past.ms <- Ht[past.idx, 2]
  
  logs.trig.comp <- theta2 + exp(theta3)*(past.ms - M0) + theta4 + 
    log(exp(theta5) + 1) +  
    (-exp(theta5) - 2)*log(tt - past.ts + exp(theta4))
  
  log.trig <- logSumExp(logs.trig.comp)
  
  -exp(log.trig)/ll0
}

log.lambda.der.th4 <- function(theta.par, ts, Ht, M0){
  sapply(ts, function(x) log.lambda.der.th4.single(theta.par, x, Ht, M0))
}



##########
# theta5 #
##########

log.lambda.der.th5.single <- function(theta.par, tt, Ht, M0){
  
  ll0 <- exp(log.lambda.ETAS(tt, theta.par, Ht, M0))
  
  theta2 <- theta.par[2]
  theta3 <- theta.par[3]
  theta4 <- theta.par[4]
  theta5 <- theta.par[5]
  
  past.idx <- Ht[,1] < tt
  past.ts <- Ht[past.idx, 1]
  past.ms <- Ht[past.idx, 2]
  
  
   logs.trig.comp <- theta2 + exp(theta3)*(past.ms - M0) + theta5 + 
     (-exp(theta5) - 1)*log(tt - past.ts + exp(theta4))
   
   sum(-exp(logs.trig.comp)*log(tt - past.ts + exp(theta4)))/ll0
}


log.lambda.der.th5 <- function(theta.par, ts, Ht, M0){
  sapply(ts, function(x) log.lambda.der.th5.single(theta.par, x, Ht, M0))
}




# univariate likelihood analysis
toplot.loglambda.lin.comp <- function(par.values, par.idx, par.name, par0,
                                      theta.par, Ht, M0, Tlim, derFUN.list,
                                      tt = 5){
  
  
  theta.m.true <- cbind(rep(theta.par[1], length(par.values)),
                        rep(theta.par[2], length(par.values)),
                        rep(theta.par[3], length(par.values)),
                        rep(theta.par[4], length(par.values)),
                        rep(theta.par[5], length(par.values)))
  
  theta.par0 <- theta.par 
    
  theta.m.true[,par.idx] <- par.values
  theta.par0[par.idx] <- par0
  
  LL.true <- sapply(1:nrow(theta.m.true), function(i) 
    log.lambda.ETAS.single(tt, theta.m.true[i,], Ht, M0))
  
  # LL.lin <- sapply(1:nrow(theta.m.true), function(i)
  #   log.lambda.ETAS.single(tt, theta.par0, Ht, M0) + 
  #     (theta.m.true[i, par.idx] - par0)*derFUN.list[[par.idx]](theta.par0,
  #                                                                 tt, Ht, M0))
  # 
  LL.lin <- sapply(1:nrow(theta.m.true), function(i) 
    log.lambda.ETAS.lin(tt, theta.m.true[i,], Ht, M0, theta.par0))
  
  
  df <- rbind(data.frame(x = par.values, 
                         y = LL.true, 
                         typ = 'correct'),
              data.frame(x = par.values, 
                         y = LL.lin, 
                         typ = 'lin approx'))
  
  list(pl = ggplot(df, aes(x = x, y = y, color = typ, 
                           linetype = typ)) + 
         geom_line() + 
         xlab(par.name) + 
         ylab('log-lambda') + 
         geom_vline(xintercept = par0, linetype = 2, color = 'darkgrey'),
       df = df)
}



## linearized log-lambda

log.lambda.ETAS.lin <- function(ts, theta.par, Ht, M0, theta.par0){
  
  ll0 <- log.lambda.ETAS(ts, theta.par0, Ht, M0)
  
  ll0 + 
    (theta.par[1] - theta.par0[1])*log.lambda.der.th1(theta.par0, ts, Ht, M0) +
    (theta.par[2] - theta.par0[2])*log.lambda.der.th2(theta.par0, ts, Ht, M0) +
    (theta.par[3] - theta.par0[3])*log.lambda.der.th3(theta.par0, ts, Ht, M0) +
    (theta.par[4] - theta.par0[4])*log.lambda.der.th4(theta.par0, ts, Ht, M0) +
    (theta.par[5] - theta.par0[5])*log.lambda.der.th5(theta.par0, ts, Ht, M0)
                      
}



# with numerical integration

ETAS.log.lik.lin1 <- function(theta.par, Ht, M0, Tlim, theta.par0, by.s){
  mesh.t <- inla.mesh.1d(loc = seq(0, Tlim, by = by.s))
  ips <- ipoints(domain = mesh.t)
  
  approx.int <- sum(exp(log.lambda.ETAS.lin(ips$x, theta.par, Ht, M0, theta.par0))*ips$weight) 
  
  approx.sum <- log.lambda.ETAS.lin(Ht[,1], theta.par, Ht, M0, theta.par0)
  
  - approx.int + sum(approx.sum)
}



toplot.loglik.lin.comp <- function(par.values, par.idx, par.name,
                             ML.est, Ht, M0, Tlim, theta.par0, by.s){
  
  
  theta.m <- cbind(rep(ML.est[1], length(par.values)),
                   rep(ML.est[2], length(par.values)),
                   rep(ML.est[3], length(par.values)),
                   rep(ML.est[4], length(par.values)),
                   rep(ML.est[5], length(par.values)))
  
  theta.m[,par.idx] <- par.values
  
  
  LL.true <- sapply(1:nrow(theta.m), function(i) 
    ETAS.log.lik(theta.m[i,], Ht, M0, Tlim))
  
  LL.approx <- sapply(1:nrow(theta.m), function(i) 
    ETAS.log.lik.lin1(theta.m[i,], Ht, M0, Tlim, theta.par0, by.s))
  
  print(length(LL.approx))
  df <- rbind(data.frame(x = par.values, 
                         y = LL.true, 
                         typ = 'correct'),
              data.frame(x = par.values, 
                         y = LL.approx, 
                         typ = 'linear approx'))
  
  ggplot(df, aes(x = x, y = y, color = typ, linetype = typ)) + 
    geom_line() + 
    xlab(par.name) + 
    ylab('log-lik') +
    geom_vline(xintercept = theta.par0[par.idx], color = 'darkgrey', linetype = 2)
}









